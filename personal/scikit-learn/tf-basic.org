#+TITLE: Getting Started With Tensorflow

* 介绍 
  TensorFlow™ is an open source software library for numerical computation 
  using data flow graphs. Nodes in the graph represent mathematical operations, 
  while the graph edges represent the multidimensional data arrays (tensors) 
  communicated between them. The flexible architecture allows you to deploy computation
  to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. 
  TensorFlow was originally developed by researchers and engineers working on the Google 
  Brain Team within Google's Machine Intelligence research organization for the purposes 
  of conducting machine learning and deep neural networks research, but the system is 
  general enough to be applicable in a wide variety of other domains as well.

  具体参加 [[https://www.tensorflow.org/][官网]] 。

* 安装

  具体可参见 [[https://www.tensorflow.org/install/][Tensorflow 官方网页]]

* 关于 Tensors 的说明

  1. The central unit of data in Tensorflow is a =tensor=.
  2. A tensor consists of a set of primitive values shaped into an array of any
     number of dimensions.
  3. A tensor's =rank= is its number of dimensions.

     #+BEGIN_EXAMPLE
       3 # rank 为 0 的张量；这是一个标量，shape 为 []
       [1., 2., 3.] # rank 为 1 的张量；这是一个向量，shape 为[3]
       [[1., 2., 3.], [4., 5., 6.]] # rank 为 2 的张量；是一个 shape 为 [2,3] 的矩阵
       [[[1., 2., 3.]], [[7., 8., 9.]]] # rank 为 3, shape 为 [2,1,3] 的张量
     #+END_EXAMPLE
* Tensorflow 核心教程

** Importing Tensorflow

   #+BEGIN_SRC ipython
     import tensorflow as tf
   #+END_SRC

** The Computational Graph
*** 构建 Tensorflow 核心编程算法主要分为两个部分

       - 构建 computational graph

       - 运行 computational graph

*** 构建 Computational Graph

    #+BEGIN_VERSE
    A computational graph is a series of Tensorflow operations arranged into a
    graph of nodes.
    #+END_VERSE

    1. 一个简单的 computational graph
       
       #+BEGIN_SRC ipython
         node1 = tf.constant(3.0, tf.float32)
         node2 = tf.constant(4.0)

         print(node1, node2)
       #+END_SRC

       #+BEGIN_EXAMPLE
         Tensor("Const_8:0", shape=(), dtype=float32) Tensor("Const_9:0", shape=(), dtype=float32)
       #+END_EXAMPLE
         
       - 需要注意的是，代码块并没有如预期地输出值 3.0 和 4.0. 相反，代码块输出了节点的信息，只有当我们去
         执行节点的时候，也就是通过开启一个 =session= 来运行 Tensorflow 的 computational graph, 节点
         才会输出正确的数值。

       - A =session= encapsulates the control and state of the Tensorflow runtime.

*** 运行 Computational Graph
       
    1. 一个简单的 =session= 实现

       #+BEGIN_SRC ipython
         sess = tf.Session()
         print(sess.run([node1, node2]))
       #+END_SRC

       #+BEGIN_EXAMPLE
         [3.0, 4.0]
       #+END_EXAMPLE

    2. 实现更为复杂的 =session= 实例
       
       #+BEGIN_SRC ipython
         node3 = tf.add(node1, node2)
         print("node3: ", node3)
         print("sess.run(node3): ", sess.run(node3))
       #+END_SRC

       #+BEGIN_EXAMPLE
         node3:  Tensor("Add:0", shape=(), dtype=float32)
         sess.run(node3):  7.0
       #+END_EXAMPLE

    3. Tensorflow 提供了一个叫做 =TensorBoard= 的工具对 computational graph 进行可视化，使用时，只需要
       加入类似下面的语句

       #+BEGIN_SRC ipython
         file_writer = tf.summary.FileWriter("/home/curiousbull/Downloads/tmp.log", sess.graph)
       #+END_SRC

       然后，执行 shell 命令：
       
       #+BEGIN_SRC bash
         tensorboard --logdir=/home/curiousbull/Downloads
       #+END_SRC
       
       打开浏览器，输入网址 'http://127.0.0.1:6006' 即可看到 TensorBoard.

    4. 利用 =placeholder=, 一个图接受来自外部的输入，a =placeholder= is a promise to provide a value latex.

       #+BEGIN_SRC ipython
         node3 = tf.add(node1, node2)
         print("node3: ", node3)
         print("sess.run(node3): ", sess.run(node3))
         a = tf.placeholder(tf.float32)
         b = tf.placeholder(tf.float32)
         adder_node = a + b # +provides a shortcut for tf.add(a,b)
         print(sess.run(adder_node, {a:3, b:4.5}))
         print(sess.run(adder_node, {a:[1,3], b:[2,4]}))
       #+END_SRC

       #+BEGIN_EXAMPLE
         7.5
         [ 3.  7.]
       #+END_EXAMPLE

    5. 我们可以通过增加其他 op 让这个 computational graph 变得更加复杂

       #+BEGIN_SRC ipython
         add_and_triple = adder_node * 3
         print(sess.run(add_and_triple, {a:3, b:4.5}))
       #+END_SRC

       #+BEGIN_EXAMPLE
         22.5
       #+END_EXAMPLE
** Model That Can Take Arbitrary Inputs 
*** Variables
    #+BEGIN_VERSE
    =Variables= allow us to add trainable parameters to a graph. 
    #+END_VERSE
    
    #+BEGIN_SRC ipython
      W = tf.Variable([.3], tf.float32)
      b = tf.Variable([-.3], tf.float32)
      x = tf.placeholder(tf.float32)
      linear_model = W*x+b
    #+END_SRC

*** Variables 的初始化以及创建 model 
    =Constants= 的值在你一使用 =tf.constant= 的时候就已经被初始化了，而且
    之后也不会改变，但是 =variables= 在使用 =tf.Variable= 的时候，并没有
    被初始化，为了初始化 =variables=, 我们需要调用一个特殊的 op：
    
    #+BEGIN_SRC ipython
      init = tf.global_variables_initializer()
      sess.run(init)
    #+END_SRC

    之后，我们可以给 =x= 赋予一些值

    #+BEGIN_SRC ipython
      print(sess.run(linear_model, {x:1,2,3,4}))
    #+END_SRC

    Output 如下：

    #+BEGIN_EXAMPLE
      [ 0.          0.30000001  0.60000002  0.90000004]
    #+END_EXAMPLE

*** 损失函数
    评价一个 model 的好坏，我们需要写一个 loss function, 并用一个 variable 来
    保存期望值。对于上述线性模型，我们用累积方差作为损失函数。具体做法如下：
    =linear_model - y= 可以创建一个对应 examples's error 向量。
    利用 =tf.square= 可以 square the error. 然后，利用 =tf.reduce_sum= 可以
    累积各项 error 的平方。

    #+BEGIN_SRC ipython
      y = tf.placeholder(tf.float32)
      squared_deltas = tf.square(linear_model-y) 
      loss = tf.reduce_sum(squared_deltas)
      print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))
    #+END_SRC

    #+BEGIN_EXAMPLE
      23.66
    #+END_EXAMPLE
    
    
*** 关于 =tf.assign=
    对于上述 model，我们可以手动修改参数 =W= 和 =b= 为 -1 和 1，
    得到最佳的参数。通过 =tf.assign= 我们可以修改 =variable=

    #+BEGIN_SRC ipython
      fixW = tf.assign(W, [-1.])
      fixb = tf.assign(b, [1.])
      sess.run([fixW,fixb])
      print(sess.run(loss,{x:[1,2,3,4],y:[0,-1,-2,-3]}))
    #+END_SRC

    得到结果为：
    
    #+BEGIN_EXAMPLE
      0.0
    #+END_EXAMPLE

** =tf.train= API
   为了获得最小的损失函数，Tensorflow 实际上提供了 =optimizers=, 其
   可以 slowly change the variable. 最简单的 =optimizer= 是
   =gradient descent=, 其根据损失函数对应变量方向的梯度改变该变量。
   一般而言，手撸偏微分符号计算是挺困难的，还好，tensorflow 可以根据对于
   model 使用函数的描述，通过 =tf.gradients= 自动进行微分。譬如，

   #+BEGIN_SRC ipython
     optimizer = tf.train.GradientDescentOptimizer(0.01)
     train = optimizer.minimize(loss)
     sess.run(init) # reset values to incorrect defaults
     for i in range(1000):
         sess.run(train, {x:[1,2,3,4], y:[0,-1,-2,-3]})
     print(sess.run([W,b]))
   #+END_SRC

   输出为：
   #+BEGIN_EXAMPLE
     [array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]
   #+END_EXAMPLE
** Complete Program
   #+BEGIN_SRC ipython
     import tensorflow as tf
     import numpy as np
     # Model Parameters
     W = tf.Variable([0.3], tf.float32)
     b = tf.Variable([-0.3], tf.float32)
     # Model input and output
     x = tf.placeholder(tf.float32)
     y = tf.placeholder(tf.float32)
     linear_model = W*x + b
     # loss function
     loss = tf.reduce_sum(tf.square(linear_model-y)) #sum of the squares
     # optimizer
     optimizer = tf.train.GradientDescentOptimizer(0.01)
     train = optimizer.minimize(loss)
     # training data
     x_train = np.array([1,2,3,4])
     y_train = np.array([0,-1,-2,-3])
     # training loop
     init = tf.global_variables_initializer()
     sess.run(init)
     for i in range(1000):
         sess.run(train, {x:x_train, y:y_train})

     # evaluate training accuracy
     curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x:x_train, y:y_train})
     print("W: %s b: %s loss: %s"%(curr_W, curr_b, curr_loss))
   #+END_SRC

   输出为：
   #+BEGIN_EXAMPLE
     W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11
   #+END_EXAMPLE
** =tf.contrib.learn=
   #+BEGIN_VERSE
   =tf.contrib.learn= is a high-level Tensorflow library that simplifies
   the mechanics of machine learning, including the following:
   - running training loops
   - running evaluating loops
   - managing data sets=
   - managing feeding
   #+END_VERSE

   1. Basic usage

      #+BEGIN_SRC ipython
        import tensorflow as tf
        import numpy as np

        # declare features
        features = [tf.contrib.layers.real_valued_column("x", dimension=1)]

        # provide an estimator that does linear regression
        estimator = tf.contrib.learn.LinearRegressor(feature_columns=features)

        # we need to tell numpy_input_fn function how many batches of data (num_epochs) we
        # want and how big each batch should be
        x = np.array([1.,2.,3.,4.])
        y = np.array([0,-1.,-2.,-3.])
        input_fn = tf.contrib.learn.io.numpy_input_fn({"x":x},y,batch_size=4,num_epochs=1000)

        # we can invoke 1000 training steps by invoking the 'fit' method and passing the
        # training data set
        estimator.fit(input_fn=input_fn, steps=1000)

        # here we evaluate how well our model did
        # in a real example, we would want to use a seperate validation and testing data set
        # to avoid overfitting
        estimator.evaluate(input_fn = input_fn)
      #+END_SRC

      When run, the output is
      #+BEGIN_EXAMPLE
        {'global_step': 1000, 'loss': 2.0945105e-08}
      #+END_EXAMPLE
** A custom model
   =tf.contrib.learn= 并不会限制我们自己写入自己的 model. 假如我们想要创建一个
   Tensorflow 没有的 model，我们依然可以保持 =tf.contrib.learn= 中的
   data set, feeding, traing 等的高度抽象。以实现之前的 =LinearRegressor=
   为例，我们利用 Tensorflow 提供的 API，来定制自己的 model.

   为了定义能够在 =tf.contrib.learn= 下工作的 =estimator=, 我们需要使用
   =tf.contrib.learn.Estimator=, 之前使用的 =tf.contrib.learn.LinearRegressor=
   其实是 =tf.contrib.learn.Estimator= 的一个子类。只需要给 =Estimator= 提供
   一个函数 =model_fn=, 告诉 =tf.contrib.learn= 它该如何 evaluate predictions,
   training steps 和 loss.

   #+BEGIN_SRC ipython
     import numpy as np
     import tensorflow as tf
     def model(features, labels, mode):
         W = tf.get_variable("W", [1], dtype=tf.float64)
         b = tf.get_variable("b", [1], dtype=tf.float64)
         y = W*features['x']+b
         # loss sub-graph
         loss = tf.reduce_sum(tf.square(y-labels))
         # training sub-graph
         global_step = tf.train.get_global_step()
         optimizer = tf.train.GradientDescentOptimizer(0.01)
         train = tf.group(optimizer.minimize(loss),
                          tf.assign_add(global_step,1))
         # ModelFnOps connects subgraphs we built to the appropriate functionality
         return tf.contrib.learn.ModelFnOps(
         mode = mode, predictions = y, loss = loss, train_op = train
         )

     estimator = tf.contrib.learn.Estimator(model_fn=model)
     # define our data set
     x = np.array([1.,2.,3.,4.])
     y = np.array([0.,-1.,-2.,-3.])
     input_fn = tf.contrib.learn.io.numpy_input_fn({"x":x},y,batch_size=4,num_epochs=1000)

     # train
     estimator.fit(input_fn=input_fn, steps=1000)
     # evaluate our model
     print(estimator.evaluate(input_fn=input_fn,steps=10))
   #+END_SRC
   
   输出为：
   #+BEGIN_EXAMPLE
     {'loss': 9.8243899e-11, 'global_step': 1000}
   #+END_EXAMPLE
